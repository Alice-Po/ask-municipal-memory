/**
 * =============================================================================
 * API ROUTE: CHATBOT RAG MUNICIPAL
 * =============================================================================
 *
 * ENDPOINT: POST /api/chat
 *
 * DESCRIPTION:
 * Cette API route impl√©mente un syst√®me de Retrieval-Augmented Generation (RAG)
 * sp√©cialis√© pour les comptes-rendus de conseils municipaux. Elle combine
 * recherche vectorielle, filtrage temporel intelligent et g√©n√©ration de texte
 * pour fournir des r√©ponses contextuelles et sourc√©es.
 *
 * ARCHITECTURE RAG:
 * 1. Embedding de la question utilisateur
 * 2. Recherche vectorielle dans Qdrant
 * 3. Recherche hybride temporelle (filtrage + pond√©ration)
 * 4. Construction du contexte pour le LLM
 * 5. G√©n√©ration de r√©ponse avec Mistral
 * 6. Enrichissement des sources avec m√©tadonn√©es
 *
 * TECHNOLOGIES UTILIS√âES:
 * - Hugging Face Inference API (embeddings + LLM)
 * - Qdrant Vector Database (stockage et recherche)
 * - Recherche hybride temporelle (propri√©taire)
 *
 * PERFORMANCE:
 * - Temps de r√©ponse typique: 2-5 secondes
 * - Limite de tokens: 512 pour la r√©ponse
 * - Chunks analys√©s: 10 maximum pour le contexte
 *
 * S√âCURIT√â:
 * - Validation des entr√©es utilisateur
 * - Gestion s√©curis√©e des cl√©s API
 * - Logging d√©taill√© pour audit
 *
 * MAINTENANCE:
 * - Logs structur√©s avec pr√©fixes [API]
 * - Gestion d'erreurs compl√®te
 * - M√©tadonn√©es de debug incluses
 *
 * =============================================================================
 */

export const prerender = false;

// =============================================================================
// IMPORTS ET CONFIGURATION
// =============================================================================

import { config } from 'dotenv';
import { InferenceClient } from '@huggingface/inference';
import { systemPrompt } from '../../prompts/systemPrompt.js';
import { performHybridSearch, logSearchMetadata } from '../../utils/temporalSearch.js';

// Chargement des variables d'environnement
config();

// =============================================================================
// CONSTANTES ET CONFIGURATION
// =============================================================================

/**
 * Configuration des mod√®les IA utilis√©s
 */
const AI_MODELS = {
  LLM: 'mistralai/Mistral-7B-Instruct-v0.2', // Mod√®le de g√©n√©ration de texte
  EMBEDDING: 'sentence-transformers/all-MiniLM-L6-v2', // Mod√®le d'embeddings (384 dimensions)
};

/**
 * Configuration de la recherche vectorielle
 */
const SEARCH_CONFIG = {
  VECTOR_LIMIT: 20, // Nombre de chunks r√©cup√©r√©s de Qdrant
  CONTEXT_LIMIT: 10, // Nombre de chunks utilis√©s pour le contexte LLM
  MAX_TOKENS: 512, // Limite de tokens pour la r√©ponse
  TEMPERATURE: 0.3, // Cr√©ativit√© du mod√®le (0 = d√©terministe, 1 = cr√©atif)
};

/**
 * Configuration de la recherche hybride temporelle
 */
const HYBRID_SEARCH_CONFIG = {
  TEMPORAL_WEIGHT: 0.3, // Poids du facteur temporel (30%)
  YEAR_TOLERANCE: 2, // Tol√©rance en ann√©es (¬±2 ans)
  ENABLE_FILTERING: true, // Activation du filtrage temporel
  ENABLE_WEIGHTING: true, // Activation de la pond√©ration temporelle
};

// =============================================================================
// FONCTION PRINCIPALE - POST /api/chat
// =============================================================================

/**
 * G√®re les requ√™tes POST pour le chatbot RAG municipal
 *
 * @async
 * @param {Object} params - Param√®tres de la requ√™te Astro
 * @param {Request} params.request - Objet Request de la requ√™te HTTP
 * @returns {Promise<Response>} R√©ponse JSON avec la r√©ponse du chatbot
 *
 * @example
 * // Requ√™te client
 * const response = await fetch('/api/chat', {
 *   method: 'POST',
 *   headers: { 'Content-Type': 'application/json' },
 *   body: JSON.stringify({ message: 'Quels sont les projets pour 2025?' })
 * });
 *
 * // R√©ponse
 * {
 *   answer: "R√©ponse g√©n√©r√©e par l'IA...",
 *   sources: [...],
 *   chunksFound: 5,
 *   searchMetadata: {...}
 * }
 */
export async function POST({ request }) {
  console.log('[API] üöÄ D√©but de la requ√™te POST /api/chat');

  try {
    // =====================================================================
    // √âTAPE 1: VALIDATION ET EXTRACTION DES DONN√âES
    // =====================================================================

    console.log('[API] üìù Validation et extraction des donn√©es...');
    const { message } = await request.json();

    // Validation de la requ√™te
    if (!message || typeof message !== 'string' || message.trim().length === 0) {
      console.log('[API] ‚ùå Message invalide ou manquant');
      return createErrorResponse(400, 'Message manquant ou invalide');
    }

    const userMessage = message.trim();
    console.log('[API] ‚úÖ Message valid√©:', userMessage.substring(0, 100) + '...');

    // =====================================================================
    // √âTAPE 2: V√âRIFICATION DES VARIABLES D'ENVIRONNEMENT
    // =====================================================================

    console.log("[API] üîß V√©rification des variables d'environnement...");
    const envVars = validateEnvironmentVariables();
    if (!envVars.valid) {
      throw new Error(`Configuration manquante: ${envVars.missing.join(', ')}`);
    }
    console.log('[API] ‚úÖ Configuration valid√©e');

    // =====================================================================
    // √âTAPE 3: G√âN√âRATION DE L'EMBEDDING
    // =====================================================================

    console.log("[API] üß† G√©n√©ration de l'embedding...");
    const hf = new InferenceClient(envVars.hfKey);

    const embeddingRes = await hf.featureExtraction({
      model: AI_MODELS.EMBEDDING,
      inputs: userMessage,
    });

    // Normalisation de la r√©ponse d'embedding
    const embedding = Array.isArray(embeddingRes) ? embeddingRes : embeddingRes[0];
    console.log(`[API] ‚úÖ Embedding g√©n√©r√© (${embedding.length} dimensions)`);

    // =====================================================================
    // √âTAPE 4: RECHERCHE VECTORIELLE DANS QDRANT
    // =====================================================================

    console.log('[API] üîç Recherche vectorielle dans Qdrant...');
    const rawChunks = await performVectorSearch(
      embedding,
      envVars.qdrantUrl,
      envVars.qdrantCollection,
      envVars.qdrantApiKey
    );

    console.log(`[API] ‚úÖ ${rawChunks.length} chunks trouv√©s dans la recherche vectorielle`);

    // =====================================================================
    // √âTAPE 5: RECHERCHE HYBRIDE TEMPORELLE
    // =====================================================================

    console.log('[API] ‚è∞ Application de la recherche hybride temporelle...');
    const { chunks: topChunks, metadata: searchMetadata } = performHybridSearch(
      rawChunks,
      userMessage,
      HYBRID_SEARCH_CONFIG
    );

    // Log des m√©tadonn√©es de recherche pour debug
    logSearchMetadata(searchMetadata, userMessage);

    // Limitation du nombre de chunks pour le contexte LLM
    const finalChunks = topChunks.slice(0, SEARCH_CONFIG.CONTEXT_LIMIT);
    console.log(`[API] ‚úÖ ${finalChunks.length} chunks s√©lectionn√©s pour le contexte`);

    // =====================================================================
    // √âTAPE 6: CONSTRUCTION DU CONTEXTE LLM
    // =====================================================================

    console.log('[API] üìö Construction du contexte pour le LLM...');
    const contextText = buildContextText(finalChunks);
    const userPrompt = buildUserPrompt(contextText, userMessage);

    // =====================================================================
    // √âTAPE 7: G√âN√âRATION DE LA R√âPONSE AVEC MISTRAL
    // =====================================================================

    console.log('[API] ü§ñ G√©n√©ration de la r√©ponse avec Mistral...');
    const llmRes = await hf.chatCompletion({
      model: AI_MODELS.LLM,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      parameters: {
        max_tokens: SEARCH_CONFIG.MAX_TOKENS,
        temperature: SEARCH_CONFIG.TEMPERATURE,
      },
    });

    const answer =
      llmRes.choices?.[0]?.message?.content || "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.";

    console.log('[API] ‚úÖ R√©ponse g√©n√©r√©e:', answer.substring(0, 100) + '...');

    // =====================================================================
    // √âTAPE 8: CONSTRUCTION DES SOURCES ENRICHIES
    // =====================================================================

    console.log('[API] üìÑ Construction des sources enrichies...');
    const sourcesWithUrls = buildEnrichedSources(finalChunks);

    // =====================================================================
    // √âTAPE 9: PR√âPARATION DE LA R√âPONSE FINALE
    // =====================================================================

    const responseData = {
      answer,
      sources: sourcesWithUrls,
      chunksFound: finalChunks.length,
      searchMetadata,
      systemPrompt,
      contextText,
      userPrompt,
    };

    console.log('[API] ‚úÖ Envoi de la r√©ponse finale');
    return createSuccessResponse(responseData);
  } catch (error) {
    // =====================================================================
    // GESTION D'ERREUR GLOBAL
    // =====================================================================

    console.error('[API] üí• Exception:', error);
    console.error('[API] üìç Stack trace:', error.stack);

    return createErrorResponse(500, 'Erreur lors du traitement de votre demande', error.message);
  }
}

// =============================================================================
// FONCTIONS UTILITAIRES
// =============================================================================

/**
 * Valide les variables d'environnement requises
 *
 * @returns {Object} Objet avec statut de validation et variables
 */
function validateEnvironmentVariables() {
  const hfKey = process.env.HUGGINGFACE_API_KEY;
  const qdrantUrl = process.env.QDRANT_URL;
  const qdrantCollection = process.env.QDRANT_COLLECTION_NAME || 'municipal_council_minutes';
  const qdrantApiKey = process.env.QDRANT_API_KEY;

  const missing = [];
  if (!hfKey) missing.push('HUGGINGFACE_API_KEY');
  if (!qdrantUrl) missing.push('QDRANT_URL');

  console.log("[API] üìã Variables d'environnement:");
  console.log('- HUGGINGFACE_API_KEY:', hfKey ? '‚úÖ PR√âSENTE' : '‚ùå MANQUANTE');
  console.log('- QDRANT_URL:', qdrantUrl || '‚ùå MANQUANTE');
  console.log('- QDRANT_COLLECTION_NAME:', qdrantCollection);
  console.log('- QDRANT_API_KEY:', qdrantApiKey ? '‚úÖ PR√âSENTE' : '‚ö†Ô∏è MANQUANTE');

  return {
    valid: missing.length === 0,
    missing,
    hfKey,
    qdrantUrl,
    qdrantCollection,
    qdrantApiKey,
  };
}

/**
 * Effectue la recherche vectorielle dans Qdrant
 *
 * @param {Array<number>} embedding - Vecteur d'embedding de la question
 * @param {string} qdrantUrl - URL de l'instance Qdrant
 * @param {string} collection - Nom de la collection
 * @param {string} apiKey - Cl√© API Qdrant (optionnelle)
 * @returns {Promise<Array>} Chunks trouv√©s avec m√©tadonn√©es
 */
async function performVectorSearch(embedding, qdrantUrl, collection, apiKey) {
  const qdrantRes = await fetch(`${qdrantUrl}/collections/${collection}/points/search`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      ...(apiKey ? { 'api-key': apiKey } : {}),
    },
    body: JSON.stringify({
      vector: embedding,
      limit: SEARCH_CONFIG.VECTOR_LIMIT,
      with_payload: true,
      with_score: true,
    }),
  });

  const qdrantData = await qdrantRes.json();

  if (!qdrantRes.ok) {
    throw new Error(qdrantData.status?.error || 'Erreur lors de la recherche Qdrant');
  }

  return (
    qdrantData.result
      ?.map((pt) => ({
        text: pt.payload?.text,
        score: pt.score,
        filename: pt.payload?.filename,
        page: pt.payload?.page_number,
        year: pt.payload?.year,
      }))
      .filter((chunk) => chunk.text) || []
  );
}

/**
 * Construit le texte de contexte pour le LLM
 *
 * @param {Array} chunks - Chunks de documents s√©lectionn√©s
 * @returns {string} Texte de contexte format√©
 */
function buildContextText(chunks) {
  return chunks
    .map((chunk) => {
      const sourceInfo = [
        chunk.filename || 'Document',
        chunk.page ? `page ${chunk.page}` : '',
        chunk.year ? `ann√©e ${chunk.year}` : '',
        chunk.temporalScore
          ? `pertinence temporelle: ${(chunk.temporalScore * 100).toFixed(1)}%`
          : '',
      ]
        .filter(Boolean)
        .join(', ');

      return `[Source: ${sourceInfo}]\n${chunk.text}`;
    })
    .join('\n---\n');
}

/**
 * Construit le prompt utilisateur pour le LLM
 *
 * @param {string} contextText - Contexte des documents
 * @param {string} userMessage - Question de l'utilisateur
 * @returns {string} Prompt complet
 */
function buildUserPrompt(contextText, userMessage) {
  return `Contexte des documents municipaux :
${contextText}

Question de l'utilisateur : ${userMessage}`;
}

/**
 * Construit les sources enrichies avec URLs et m√©tadonn√©es
 *
 * @param {Array} chunks - Chunks de documents
 * @returns {Array} Sources avec URLs et m√©tadonn√©es temporelles
 */
function buildEnrichedSources(chunks) {
  return chunks.map((chunk) => {
    // Construction de l'URL du PDF
    let pdfUrl = null;
    if (chunk.filename && chunk.year) {
      pdfUrl = `/datas/${chunk.year}/${chunk.filename}`;
    } else if (chunk.filename) {
      // Fallback : extraction de l'ann√©e du filename
      const yearMatch = chunk.filename.match(/(\d{4})/);
      if (yearMatch) {
        const year = yearMatch[1];
        pdfUrl = `/datas/${year}/${chunk.filename}`;
      }
    }

    return {
      filename: chunk.filename,
      page: chunk.page,
      year: chunk.year,
      score: chunk.finalScore || chunk.score,
      originalScore: chunk.originalScore || chunk.score,
      temporalScore: chunk.temporalScore,
      url: pdfUrl,
      urlWithPage: chunk.page && pdfUrl ? `${pdfUrl}#page=${chunk.page}` : pdfUrl,
    };
  });
}

/**
 * Cr√©e une r√©ponse de succ√®s
 *
 * @param {Object} data - Donn√©es de la r√©ponse
 * @returns {Response} R√©ponse HTTP
 */
function createSuccessResponse(data) {
  return new Response(JSON.stringify(data), {
    status: 200,
    headers: {
      'Content-Type': 'application/json',
      'Cache-Control': 'no-cache',
    },
  });
}

/**
 * Cr√©e une r√©ponse d'erreur
 *
 * @param {number} status - Code de statut HTTP
 * @param {string} message - Message d'erreur
 * @param {string} details - D√©tails optionnels
 * @returns {Response} R√©ponse HTTP d'erreur
 */
function createErrorResponse(status, message, details = null) {
  const errorData = { error: message };
  if (details) errorData.details = details;

  return new Response(JSON.stringify(errorData), {
    status,
    headers: { 'Content-Type': 'application/json' },
  });
}

// =============================================================================
// GESTION DES M√âTHODES HTTP NON SUPPORT√âES
// =============================================================================

/**
 * G√®re les requ√™tes GET (non support√©es)
 *
 * @returns {Response} R√©ponse d'erreur 405
 */
export async function GET() {
  return createErrorResponse(405, 'M√©thode GET non support√©e. Utilisez POST.');
}

/**
 * G√®re les requ√™tes PUT (non support√©es)
 *
 * @returns {Response} R√©ponse d'erreur 405
 */
export async function PUT() {
  return createErrorResponse(405, 'M√©thode PUT non support√©e. Utilisez POST.');
}

/**
 * G√®re les requ√™tes DELETE (non support√©es)
 *
 * @returns {Response} R√©ponse d'erreur 405
 */
export async function DELETE() {
  return createErrorResponse(405, 'M√©thode DELETE non support√©e. Utilisez POST.');
}

/**
 * G√®re les requ√™tes PATCH (non support√©es)
 *
 * @returns {Response} R√©ponse d'erreur 405
 */
export async function PATCH() {
  return createErrorResponse(405, 'M√©thode PATCH non support√©e. Utilisez POST.');
}
